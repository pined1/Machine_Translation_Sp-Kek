# -*- coding: utf-8 -*-
"""IBM_2_Transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dPxpEFvk1qUXlLWhjqKxSzbVQ2lcjLYZ

## IBM Model 2

### Install NLTK and required packages
"""

!pip install nltk --quiet
import nltk
nltk.download('punkt')
nltk.download('perluniprops')

from nltk.translate import IBMModel2
from nltk.translate.api import AlignedSent
import os
import pickle

"""### Load aligned sentence pairs"""

file_path = "/content/aligned_sentences_clean.txt"

if not os.path.exists(file_path):
    raise FileNotFoundError(f" File not found: {file_path}")

spanish_sentences_clean = []
kekchi_sentences_clean = []

with open(file_path, "r", encoding="utf-8") as f:
    lines = f.readlines()
    for i in range(0, len(lines), 3):
        if i + 1 < len(lines):
            sp = lines[i].replace("Spanish:", "").strip()
            kq = lines[i + 1].replace("Kekchi:", "").strip()
            spanish_sentences_clean.append(sp.split())
            kekchi_sentences_clean.append(kq.split())

print(f"Loaded {len(spanish_sentences_clean)} aligned sentence pairs.")

"""### Format for IBMModel2 (Kekchi = source, Spanish = target)"""

aligned_sentences = [AlignedSent(sp, kq) for sp, kq in zip(spanish_sentences_clean, kekchi_sentences_clean)]

aligned_sentences[:5]

"""### Train IBM Model 2"""

print("Training IBM Model 2 for 20 iterations...")
ibm2 = IBMModel2(aligned_sentences, iterations=20)
print("IBM Model 2 training complete!")

ibm2

"""### Save the model for future use"""

!pip install dill

import dill

model_path = "/content/ibm_model2_trained.pkl"
with open(model_path, "wb") as f:
    dill.dump(ibm2, f)

print(f"Saved trained model to '{model_path}'")

with open(model_path, "rb") as f:
    ibm2 = dill.load(f)

"""## Use IBM Model 2 to Extract Gloss Triples"""

!ls -lh /content/ibm_model2_trained.pkl

"""### Aligned Sentences"""

def load_aligned_sentences(file_path):
    spanish = []
    kekchi = []

    with open(file_path, "r", encoding="utf-8") as f:
        lines = [line.strip() for line in f if line.strip()]

    for i in range(0, len(lines) - 1, 2):  # Stop before last line if odd
        if lines[i].startswith("Spanish:") and lines[i+1].startswith("Kekchi:"):
            sp = lines[i].replace("Spanish: ", "").strip()
            ke = lines[i+1].replace("Kekchi: ", "").strip()
            spanish.append(sp.split())
            kekchi.append(ke.split())
        else:
            print(f"Skipping unpaired or malformed lines at {i}: {lines[i:i+2]}")

    return list(zip(spanish, kekchi))

"""### Build Gloss Dictionary

* IBM Model 2 to extract common glosses (alignment-based translations):
"""

from collections import defaultdict

def build_gloss_dict(ibm2, sentence_pairs, min_count=3):
    from collections import defaultdict
    gloss_dict = defaultdict(list)

    for spanish, kekchi in sentence_pairs:
        aligned = AlignedSent(kekchi, spanish)
        ibm2.align(aligned)
        for src_idx, tgt_idx in aligned.alignment:
            if src_idx is None or tgt_idx is None:
                continue
            if tgt_idx < len(spanish) and src_idx < len(kekchi):
                gloss_dict[spanish[tgt_idx]].append(kekchi[src_idx])

    cleaned_gloss = {}
    for word, matches in gloss_dict.items():
        if len(matches) >= min_count:
            cleaned_gloss[word] = max(set(matches), key=matches.count)

    return cleaned_gloss

"""### Construct IGT Gloss Triples and Format for Pretraining"""

import json

def make_igt_jsonl(ibm2, sentence_pairs, gloss_dict, output_path):
    with open(output_path, "w", encoding="utf-8") as f_out:
        for spanish, kekchi in sentence_pairs:
            aligned = AlignedSent(kekchi, spanish)
            ibm2.align(aligned)

            gloss_line = [gloss_dict.get(word, word) for word in spanish]

            input_text = "[SRC] " + " ".join(spanish) + " [GLOSS] " + " ".join(gloss_line)
            target_text = " ".join(kekchi)

            json_obj = {
                "input": input_text,
                "target": target_text
            }
            f_out.write(json.dumps(json_obj, ensure_ascii=False) + "\n")

"""### Load"""

sentence_pairs = load_aligned_sentences("/content/aligned_sentences_clean.txt")

sentence_pairs[:5]

"""### Build Gloss Dict"""

gloss_dict = build_gloss_dict(ibm2, sentence_pairs, min_count=3)

list(gloss_dict.items())[:10]

"""### Write JSONL"""

make_igt_jsonl(ibm2, sentence_pairs, gloss_dict, "mbart_igt_pretrain_v2.jsonl")

"""### Output"""

!head mbart_igt_pretrain.jsonl

"""## Train on Gloss Dictionary

### Function for all evaluation
"""

!pip install evaluate
!pip install unbabel-comet
!pip install numpy

!pip uninstall -y numpy
!pip install numpy --upgrade --force-reinstall

!pip install numpy==1.23.5

!pip install --force-reinstall evaluate unbabel-comet

!pip install rouge_score

import numpy as np
from evaluate import load
from comet import download_model, load_from_checkpoint

def evaluate_mt_model(trainer, tokenizer, tokenized_test, raw_sources):
    print("üîç Running model predictions...")
    preds = trainer.predict(tokenized_test)
    decoded_preds = tokenizer.batch_decode(preds.predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(preds.label_ids, skip_special_tokens=True)

    print("üìè Computing BLEU, ChrF++, ROUGE, and Exact Match...")
    bleu = load("sacrebleu")
    chrf = load("chrf")
    rouge = load("rouge")

    bleu_score = bleu.compute(predictions=decoded_preds,
                              references=[[ref] for ref in decoded_labels])["score"]
    chrf_score = chrf.compute(predictions=decoded_preds,
                              references=decoded_labels)["score"]
    rouge_score = rouge.compute(predictions=decoded_preds,
                                references=decoded_labels)["rougeL"]

    exact_matches = [pred.strip() == label.strip() for pred, label in zip(decoded_preds, decoded_labels)]
    exact_match_score = sum(exact_matches) / len(exact_matches) * 100

    print("üß† Computing COMET score...")
    model_path = download_model("Unbabel/wmt22-comet-da")
    comet_model = load_from_checkpoint(model_path)

    comet_data = [
        {"src": raw_sources[i], "mt": decoded_preds[i], "ref": decoded_labels[i]}
        for i in range(len(decoded_preds))
    ]
    comet_score = comet_model.predict(comet_data, batch_size=8, gpus=1)
    comet_mean = np.mean(comet_score.scores)

    print("\n‚úÖ Evaluation Summary:")
    print(f"BLEU:        {bleu_score:.2f}")
    print(f"ChrF++:      {chrf_score:.2f}")
    print(f"ROUGE-L:     {rouge_score:.2f}")
    print(f"Exact Match: {exact_match_score:.2f}%")
    print(f"COMET:       {comet_mean:.4f}")

    return {
        "BLEU": bleu_score,
        "ChrF++": chrf_score,
        "ROUGE-L": rouge_score,
        "Exact Match (%)": exact_match_score,
        "COMET": comet_mean
    }

"""### Load the JSONL into a HuggingFace Dataset"""

from datasets import load_dataset

dataset = load_dataset("json", data_files="/content/mbart_igt_pretrain_v2.jsonl", split="train")
dataset = dataset.train_test_split(test_size=0.1)

train_data = dataset["train"]
test_data = dataset["test"]

print(test_data.features)

train_data[:5]

test_data[:5]

"""### Load Pretrained Tokenizer & Model (e.g., mbart-large-50)"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = "facebook/mbart-large-50"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

"""### Tokenize the Dataset"""

def preprocess_function(examples):
    inputs = ["translate Spanish to Kekchi: " + x for x in examples["input"]]
    targets = examples["target"]

    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding="max_length")
    labels = tokenizer(targets, max_length=128, truncation=True, padding="max_length")

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_train = train_data.map(preprocess_function, batched=True)
tokenized_test = test_data.map(preprocess_function, batched=True)

"""### ALL data"""

from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=16,
    gradient_accumulation_steps=2,
    max_steps=3000,
    eval_steps=500,
    save_steps=500,
    save_total_limit=1,
    logging_steps=100,
    evaluation_strategy="steps",
    predict_with_generate=True,
    fp16=True,
    report_to="none"
)

from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test.select(range(1000)),
    tokenizer=tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model)
)

trainer.train()

"""### Evaluation Score"""

raw_sources = [
    x.split("[GLOSS]")[0].replace("[SRC]", "").strip()
    for x in test_data["input"]
]


evaluate_mt_model(trainer, tokenizer, tokenized_test.select(range(1000)), raw_sources[:1000])