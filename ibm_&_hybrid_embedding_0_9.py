# -*- coding: utf-8 -*-
"""IBM_&_Hybrid_Embedding_0.9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/149Vradz-Nc5HrlCLYGRBx7GLu6bmlSY_

## mBART Pretraining with Hybrid Gaussian Random Embeddings for Low-Resource IGT

### Predefined function
"""

!pip uninstall -y numpy evaluate unbabel-comet rouge_score


!pip install numpy==1.23.5


!pip install evaluate unbabel-comet rouge_score

!pip install --force-reinstall evaluate unbabel-comet

import numpy as np
from evaluate import load
from comet import download_model, load_from_checkpoint

def evaluate_mt_model(trainer, tokenizer, tokenized_test, raw_sources):
    print("üîç Running model predictions...")
    preds = trainer.predict(tokenized_test)
    decoded_preds = tokenizer.batch_decode(preds.predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(preds.label_ids, skip_special_tokens=True)

    print("üìè Computing BLEU, ChrF++, ROUGE, and Exact Match...")
    bleu = load("sacrebleu")
    chrf = load("chrf")
    rouge = load("rouge")

    bleu_score = bleu.compute(predictions=decoded_preds,
                              references=[[ref] for ref in decoded_labels])["score"]
    chrf_score = chrf.compute(predictions=decoded_preds,
                              references=decoded_labels)["score"]
    rouge_score = rouge.compute(predictions=decoded_preds,
                                references=decoded_labels)["rougeL"]

    exact_matches = [pred.strip() == label.strip() for pred, label in zip(decoded_preds, decoded_labels)]
    exact_match_score = sum(exact_matches) / len(exact_matches) * 100

    print("üß† Computing COMET score...")
    model_path = download_model("Unbabel/wmt22-comet-da")
    comet_model = load_from_checkpoint(model_path)

    comet_data = [
        {"src": raw_sources[i], "mt": decoded_preds[i], "ref": decoded_labels[i]}
        for i in range(len(decoded_preds))
    ]
    comet_score = comet_model.predict(comet_data, batch_size=8, gpus=1)
    comet_mean = np.mean(comet_score.scores)

    print("\n‚úÖ Evaluation Summary:")
    print(f"BLEU:        {bleu_score:.2f}")
    print(f"ChrF++:      {chrf_score:.2f}")
    print(f"ROUGE-L:     {rouge_score:.2f}")
    print(f"Exact Match: {exact_match_score:.2f}%")
    print(f"COMET:       {comet_mean:.4f}")

    return {
        "BLEU": bleu_score,
        "ChrF++": chrf_score,
        "ROUGE-L": rouge_score,
        "Exact Match (%)": exact_match_score,
        "COMET": comet_mean
    }

"""### Install and Import Required Libraries"""

!pip install transformers datasets sentencepiece accelerate

import torch
import json
import numpy as np
from datasets import load_dataset
from transformers import MBartTokenizer, MBartForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer

"""### Load Interlinear Glossed Text (IGT) Data
* Spanish, Gloss, and Kekchi triples
"""

from datasets import load_dataset

dataset = load_dataset("json", data_files="/content/mbart_igt_pretrain_v2.jsonl", split="train")


dataset = dataset.train_test_split(test_size=0.1, seed=42)


train_data = dataset["train"]
test_data = dataset["test"]

train_data[:5]

"""### Load mBART-50 Model and Tokenizer
* planning - modifying its output embeddings.
"""

model_name = "facebook/mbart-large-50"
tokenizer = MBartTokenizer.from_pretrained(model_name)
model = MBartForConditionalGeneration.from_pretrained(model_name)

"""### Hybrid Embedding Injection (Œ≥ * pretrained + (1 - Œ≥) * random)
* For each token, compute a weighted average of its pretrained vector and a random Gaussian vector, then normalize it.

* Use a fixed Œ≥ value (e.g., 0.9 as used in the paper)
"""

def inject_hybrid_embeddings(model, gamma=0.9):
    embedding = model.model.shared.weight.data
    vocab_size, dim = embedding.shape

    for idx in range(vocab_size):
        pretrained_vec = embedding[idx]
        rand_vec = torch.randn(dim)
        rand_vec /= rand_vec.norm()

        combined = gamma * pretrained_vec + (1 - gamma) * rand_vec
        combined /= combined.norm()
        embedding[idx] = combined

inject_hybrid_embeddings(model, gamma=0.9)

"""* This ensures that rare and frequent tokens are perturbed while preserving some semantic structure from the pretrained model.

### Preprocess IGT Dataset (Tokenize Inputs and Targets)
"""

def preprocess_function(examples):
    inputs = ["translate Spanish to Kekchi: " + x for x in examples["input"]]
    targets = examples["target"]

    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding="max_length")
    labels = tokenizer(targets, max_length=128, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_train = train_data.map(preprocess_function, batched=True)
tokenized_test = test_data.map(preprocess_function, batched=True)

"""### ALL Data"""

from transformers import Seq2SeqTrainingArguments, DataCollatorForSeq2Seq

training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=16,
    gradient_accumulation_steps=2,
    max_steps=3000,
    eval_steps=500,
    save_steps=500,
    save_total_limit=1,
    logging_steps=100,
    evaluation_strategy="steps",
    predict_with_generate=True,
    fp16=True,
    report_to="none"
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test.select(range(1000)),
    tokenizer=tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model)
)

trainer.train()

"""### Evaluation Score"""

!pip install rouge_score

raw_sources = [
    x.split("[GLOSS]")[0].replace("[SRC]", "").strip()
    for x in test_data["input"]
]


evaluate_mt_model(trainer, tokenizer, tokenized_test.select(range(1000)), raw_sources[:1000])