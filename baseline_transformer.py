# -*- coding: utf-8 -*-
"""Baseline_Transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eOGlu9JzCLEQKhH3bU-XEffcc7-uotmq
"""

!pip install transformers datasets tokenizers accelerate

"""## Original  Dataset aligned sentences"""

from google.colab import files

uploaded = files.upload()  # aligned_sentences_clean.txt

file_path = "/content/aligned_sentences_clean.txt"

"""## Parse the alternating lines into pairs"""

spanish_sentences = []
kekchi_sentences = []

with open("/content/aligned_sentences_clean.txt", "r", encoding="utf-8") as f:
    lines = [line.strip() for line in f if line.strip()]

for i in range(0, len(lines), 2):
    if lines[i].startswith("Spanish:") and lines[i+1].startswith("Kekchi:"):
        spanish = lines[i].replace("Spanish:", "").strip()
        kekchi = lines[i+1].replace("Kekchi:", "").strip()
        spanish_sentences.append(spanish)
        kekchi_sentences.append(kekchi)

print(f"Loaded {len(spanish_sentences)} sentence pairs.")

"""## Convert into JSONL format (for HuggingFace)"""

import json

with open("spanish_kekchi.jsonl", "w", encoding="utf-8") as f:
    for s, k in zip(spanish_sentences, kekchi_sentences):
        json_line = {"source": s, "target": k}
        f.write(json.dumps(json_line, ensure_ascii=False) + "\n")

"""##  Load the JSONL into a HuggingFace Dataset"""

from datasets import load_dataset

dataset = load_dataset("json", data_files="spanish_kekchi.jsonl", split="train")
dataset = dataset.train_test_split(test_size=0.1)

train_data = dataset["train"]
test_data = dataset["test"]

"""## Load Pretrained Tokenizer & Model (e.g., mbart-large-50)"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = "facebook/mbart-large-50"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

"""## Tokenize the Dataset"""

def preprocess_function(examples):
    inputs = ["translate Spanish to Kekchi: " + ex for ex in examples["source"]]
    targets = examples["target"]

    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding="max_length")
    labels = tokenizer(targets, max_length=128, truncation=True, padding="max_length")

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_train = train_data.map(preprocess_function, batched=True)
tokenized_test = test_data.map(preprocess_function, batched=True)

"""##  Define Training Arguments"""

from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,               # Just 3 quick passes
    fp16=True,                        # Mixed precision for speed (if GPU)
    logging_dir="./logs",
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=1,
    report_to="none",
    predict_with_generate=True
)

"""## Start Training with Seq2SeqTrainer"""

from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    tokenizer=tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model)
)

trainer.train()

"""### subset of data"""

tokenized_train = tokenized_train.select(range(500))  # Only 500 training examples
tokenized_test = tokenized_test.select(range(100))    # Only 100 for testing

from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    tokenizer=tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model)
)

trainer.train()

"""### Evaluate Model on the Test Set"""

results = trainer.evaluate()
print(results)

"""###  Compute BLEU Score on Predictions"""

!pip install evaluate sacrebleu --quiet

from evaluate import load
import numpy as np

bleu = load("sacrebleu")

# Get model predictions
preds = trainer.predict(tokenized_test)
decoded_preds = tokenizer.batch_decode(preds.predictions, skip_special_tokens=True)
decoded_labels = tokenizer.batch_decode(preds.label_ids, skip_special_tokens=True)

# Prepare references in expected format
bleu_score = bleu.compute(predictions=decoded_preds,
                          references=[[ref] for ref in decoded_labels])

print(f"\nüåç BLEU score: {bleu_score['score']:.2f}")

"""### Comet Score"""

!pip install -q unbabel-comet

from comet import download_model, load_from_checkpoint


model_path = download_model("Unbabel/wmt22-comet-da")
comet_model = load_from_checkpoint(model_path)

"""### Comet Inputs"""

comet_data = [
    {"src": tokenized_test[i]["source"],
     "mt": decoded_preds[i],
     "ref": decoded_labels[i]}
    for i in range(len(decoded_preds))
]

"""### Comet scoring"""

import numpy as np

comet_score = comet_model.predict(comet_data, batch_size=8, gpus=1)
mean_score = np.mean(comet_score.scores)
print(f"‚ö° COMET score: {mean_score:.4f}")

"""### ChrF++"""

chrf = load("chrf")
chrf_score = chrf.compute(predictions=decoded_preds, references=decoded_labels)
print(f"ChrF++ score: {chrf_score['score']:.2f}")

"""### ROUGE-L"""

!pip install rouge_score --quiet

from evaluate import load

rouge = load("rouge")
rouge_score = rouge.compute(predictions=decoded_preds, references=decoded_labels)
print(f"ROUGE-L score: {rouge_score['rougeL']:.2f}")

"""### Exact Match"""

exact_matches = [pred.strip() == label.strip() for pred, label in zip(decoded_preds, decoded_labels)]
exact_match_score = sum(exact_matches) / len(exact_matches) * 100
print(f"Exact Match: {exact_match_score:.2f}%")

"""## Function for all the scores"""

def evaluate_mt_model(trainer, tokenizer, tokenized_test):
    from evaluate import load
    from comet import download_model, load_from_checkpoint
    import numpy as np

    print("üîç Running model predictions...")
    preds = trainer.predict(tokenized_test)
    decoded_preds = tokenizer.batch_decode(preds.predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(preds.label_ids, skip_special_tokens=True)

    print("üìè Computing BLEU, ChrF++, ROUGE, and Exact Match...")
    bleu = load("sacrebleu")
    chrf = load("chrf")
    rouge = load("rouge")

    bleu_score = bleu.compute(predictions=decoded_preds,
                              references=[[ref] for ref in decoded_labels])["score"]
    chrf_score = chrf.compute(predictions=decoded_preds,
                              references=decoded_labels)["score"]
    rouge_score = rouge.compute(predictions=decoded_preds,
                                references=decoded_labels)["rougeL"]

    exact_matches = [pred.strip() == label.strip() for pred, label in zip(decoded_preds, decoded_labels)]
    exact_match_score = sum(exact_matches) / len(exact_matches) * 100

    print("üß† Computing COMET score...")
    model_path = download_model("Unbabel/wmt22-comet-da")
    comet_model = load_from_checkpoint(model_path)

    comet_data = [
        {"src": tokenized_test[i]["source"], "mt": decoded_preds[i], "ref": decoded_labels[i]}
        for i in range(len(decoded_preds))
    ]
    comet_score = comet_model.predict(comet_data, batch_size=8, gpus=1)
    comet_mean = np.mean(comet_score.scores)

    print("\n‚úÖ Evaluation Summary:")
    print(f"BLEU:        {bleu_score:.2f}")
    print(f"ChrF++:      {chrf_score:.2f}")
    print(f"ROUGE-L:     {rouge_score:.2f}")
    print(f"Exact Match: {exact_match_score:.2f}%")
    print(f"COMET:       {comet_mean:.4f}")

    return {
        "BLEU": bleu_score,
        "ChrF++": chrf_score,
        "ROUGE-L": rouge_score,
        "Exact Match (%)": exact_match_score,
        "COMET": comet_mean
    }

"""## Full data faster"""

from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=16,         # Tune if needed (try 8 or 32 too)
    gradient_accumulation_steps=2,          # Simulates larger batch
    max_steps=3000,                         # ‚úÖ Train for ~3000 updates (not full epoch)
    eval_steps=500,
    save_steps=500,
    save_total_limit=1,
    logging_steps=100,
    evaluation_strategy="steps",
    predict_with_generate=True,
    fp16=True,                              # Mixed precision for speed
    report_to="none"
)

from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test.select(range(1000)),
    tokenizer=tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model)
)

trainer.train()

"""## Evaluate"""

evaluate_mt_model(trainer, tokenizer, tokenized_test.select(range(1000)))

